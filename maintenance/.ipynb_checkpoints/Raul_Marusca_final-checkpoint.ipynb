{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d297ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn import model_selection\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "from sklearn.metrics import f1_score, classification_report, plot_roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn import tree\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "sns.set(rc={'figure.figsize':(11,10)})\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab852d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion auxiliar\n",
    "def make_confusion_matriz(cf,\n",
    "                          group_names=None,\n",
    "                          categories='auto',\n",
    "                          count=True,\n",
    "                          percent=True,\n",
    "                          cbar=True,\n",
    "                          xyticks=True,\n",
    "                          xyplotlabels=True,\n",
    "                          sum_stats=True,\n",
    "                          figsize=None,\n",
    "                          cmap='Blues',\n",
    "                          title=None):\n",
    "    '''\n",
    "    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cf:            confusion matrix to be passed in\n",
    "\n",
    "    group_names:   List of strings that represent the labels row by row to be shown in each square.\n",
    "\n",
    "    categories:    List of strings containing the categories to be displayed on the x,y axis. Default is 'auto'\n",
    "\n",
    "    count:         If True, show the raw number in the confusion matrix. Default is True.\n",
    "\n",
    "    normalize:     If True, show the proportions for each category. Default is True.\n",
    "\n",
    "    cbar:          If True, show the color bar. The cbar values are based off the values in the confusion matrix.\n",
    "                   Default is True.\n",
    "\n",
    "    xyticks:       If True, show x and y ticks. Default is True.\n",
    "\n",
    "    xyplotlabels:  If True, show 'True Label' and 'Predicted Label' on the figure. Default is True.\n",
    "\n",
    "    sum_stats:     If True, display summary statistics below the figure. Default is True.\n",
    "\n",
    "    figsize:       Tuple representing the figure size. Default will be the matplotlib rcParams value.\n",
    "\n",
    "    cmap:          Colormap of the values displayed from matplotlib.pyplot.cm. Default is 'Blues'\n",
    "                   See http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                   \n",
    "    title:         Title for the heatmap. Default is None.\n",
    "\n",
    "    '''\n",
    "\n",
    "\n",
    "    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n",
    "    blanks = ['' for i in range(cf.size)]\n",
    "\n",
    "    if group_names and len(group_names)==cf.size:\n",
    "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
    "    else:\n",
    "        group_labels = blanks\n",
    "\n",
    "    if count:\n",
    "        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n",
    "    else:\n",
    "        group_counts = blanks\n",
    "\n",
    "    if percent:\n",
    "        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()/np.sum(cf)]\n",
    "    else:\n",
    "        group_percentages = blanks\n",
    "\n",
    "    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n",
    "    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n",
    "\n",
    "\n",
    "    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n",
    "    if sum_stats:\n",
    "        #Accuracy is sum of diagonal divided by total observations\n",
    "        accuracy  = np.trace(cf) / float(np.sum(cf))\n",
    "\n",
    "        #if it is a binary confusion matrix, show some more stats\n",
    "        if len(cf)==2:\n",
    "            #Metrics for Binary Confusion Matrices\n",
    "            precision = cf[1,1] / sum(cf[:,1])\n",
    "            recall    = cf[1,1] / sum(cf[1,:])\n",
    "            f1_score  = 2*precision*recall / (precision + recall)\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n",
    "                accuracy,precision,recall,f1_score)\n",
    "        else:\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n",
    "    else:\n",
    "        stats_text = \"\"\n",
    "\n",
    "\n",
    "    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n",
    "    if figsize==None:\n",
    "        #Get default figure size if not set\n",
    "        figsize = plt.rcParams.get('figure.figsize')\n",
    "\n",
    "    if xyticks==False:\n",
    "        #Do not show categories if xyticks is False\n",
    "        categories=False\n",
    "\n",
    "\n",
    "    # MAKE THE HEATMAP VISUALIZATION\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cf,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n",
    "\n",
    "    if xyplotlabels:\n",
    "        plt.ylabel('Valores Reales')\n",
    "        plt.xlabel('Valores Predichos' + stats_text)\n",
    "    else:\n",
    "        plt.xlabel(stats_text)\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b87f17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"mantenimiento.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd299ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e75aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedb367d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357677b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necesitamos hacer\n",
    "# 1 Descartar UDI y Product ID\n",
    "# 2 Hacer numerica Type L=0, M=1, H=2\n",
    "# 3 Hacer numerica Failure Type\n",
    "# 4 Separar Features y Target como target1 y Failure Type como target2\n",
    "# 5 Escalar el resto como features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637bf334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos los valores distintos de Failure Type\n",
    "df['Failure Type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e99130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos los valores distintos de Failure Type\n",
    "df['Type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e44e827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos los valores distintos de Failure Type\n",
    "df['Target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acad86f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Descartar UDI y Product ID\n",
    "df.drop(df.columns[:2],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38d2530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Hacer numerica Type L=0, M=1, H=2\n",
    "# 3 Hacer numerica Failure Type\n",
    "\n",
    "dict_reemplazo = {\"Type\":     {\"L\": 0, \"M\": 1, 'H': 2},\n",
    "                   \"Failure Type\":     {'No Failure' : 0,\n",
    "                                        'Power Failure' : 1,\n",
    "                                        'Tool Wear Failure' : 2,\n",
    "                                        'Overstrain Failure' : 3,\n",
    "                                        'Random Failures' : 4,\n",
    "                                        'Heat Dissipation Failure' : 5}\n",
    "               }\n",
    "# Aplicamos el reemplazo en el dataframe\n",
    "df = df.replace(dict_reemplazo)\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c65173d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos los valores distintos de Failure Type\n",
    "df['Type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30085330",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af818a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 Separar Features y Target como target1 y Failure Type como target2\n",
    "X = df.iloc[:,0:-2]\n",
    "y_tg1 = df.iloc[:,-2:-1]\n",
    "y_tg2 = df.iloc[:,-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c3e98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.pairplot(df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0566ad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Target', data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9ef74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Failure Type', data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bb1f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contamos los valores distintos\n",
    "df['Target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33132f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contamos los valores distintos\n",
    "df['Failure Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94980e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a042a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4eef7a",
   "metadata": {},
   "source": [
    "Vamos a analizar con falla/no falla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abffe7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para escalar \n",
    "# Normalizamos todo el dataset\n",
    "X_normal=(X - X.mean()) / X.std()\n",
    "X_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22243a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos PCA\n",
    "#Componentes principales\n",
    "pca = PCA()\n",
    "pca.fit(X_normal,y_tg1)\n",
    "x_new = pca.transform(X_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4bfc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Varianza explicada por cada componente\n",
    "var_pc = pca.explained_variance_ratio_\n",
    "#print(var_pc)\n",
    "\n",
    "indice = 1\n",
    "for var in var_pc:\n",
    "    print(\"PCA{} : {}\".format(indice, round(var,4)))\n",
    "    indice += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d842fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos los nombres de las columnas\n",
    "nombres = list(X_normal.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72b7c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colors\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "green_patch = mpatches.Patch(color='green', label='No Falla')\n",
    "red_patch = mpatches.Patch(color='red', label='Falla')\n",
    "\n",
    "\n",
    "# asignamos los dos primeros PC a los ejes coordinados\n",
    "x_valor = x_new[:,0]\n",
    "y_valor = x_new[:,1]\n",
    "\n",
    "cmap = colors.ListedColormap(['green', 'red'])\n",
    "\n",
    "dot = list(y_tg1.Target.values)\n",
    "\n",
    "# hacemos un grafico de dispersion\n",
    "dispersion_1 = plt.scatter(x_valor, y_valor , c=dot, cmap=cmap)\n",
    "\n",
    "# Le ponemos nombre a los ejes\n",
    "plt.xlabel(\"PC1\");\n",
    "plt.ylabel(\"PC2\");\n",
    "\n",
    "# mostramos la leyenda\n",
    "plt.legend(handles=[green_patch, red_patch], fontsize=12)\n",
    "\n",
    "# Mostamos la grilla\n",
    "plt.grid();\n",
    "\n",
    "# imprimimos el grafico completo\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb7940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el rango de los ejes del grafico\n",
    "plt.axis([-0.4,1,-0.75,0.75])\n",
    "\n",
    "# Vemos cuantos vectores son las direcciones de maxima varianza\n",
    "n = pca.components_.shape[0]\n",
    "\n",
    "# Recorremos esos vectores y los vamos dibujando en el plano\n",
    "for i in range(n):\n",
    "    plt.arrow(0, 0, pca.components_[i,0], pca.components_[i,1], color = 'r', alpha = 1);\n",
    "    # En el extremo de cada vector ponemos en nombre de la columan correspondiente (un poco dezplazados)\n",
    "    plt.text(pca.components_[i,0]*1.1 , pca.components_[i,1]*1.1, nombres[i], color = 'g', ha = 'center', va = 'center', fontsize=12);\n",
    "\n",
    "# Le ponemos nombre a los ejes\n",
    "plt.xlabel(\"PC1\");\n",
    "plt.ylabel(\"PC2\");\n",
    "\n",
    "# imprimimos el grafico completo\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f9d08e",
   "metadata": {},
   "source": [
    "Clasificacion Supervisada\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206f5043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normal, y_tg1, stratify=y_tg1, test_size=0.2, random_state=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c109a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probamos con regresion logistica\n",
    "lg = LogisticRegression() \n",
    "modelo_lg = lg.fit(X_train, y_train) \n",
    "y_pred_test = modelo_lg.predict(X_test) \n",
    "\n",
    "# matriz de confusión\n",
    "conf = confusion_matrix(y_test,y_pred_test)\n",
    "\n",
    "# grafico matriz de confusión\n",
    "labels = ['VN','FP','FN','VP']\n",
    "categories = ['No Falla', 'Falla']\n",
    "make_confusion_matriz(conf, \n",
    "                     group_names=labels,\n",
    "                     categories=categories, \n",
    "                     cmap='magma',\n",
    "                     cbar=False,\n",
    "                     sum_stats=False,\n",
    "                     figsize=(6,6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e31225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funciona solamente para clasificación binaria (2 clases)\n",
    "# para muchas clases se puede ver esto: \n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py\n",
    "plot_roc_curve(modelo_lg, X_test, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44963b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predigo probabilidades según el modelo con los datos de TESTEO.\n",
    "# Luego uso estas probabilidades para, mediante punto de corte, clasificar en clases.\n",
    "probas = modelo_lg.predict_proba(X_test)\n",
    "\n",
    "# array con los puntos de corte\n",
    "puntos_corte = np.arange(0.02, 1, 0.02) # empieza, termina, paso\n",
    "acc = [] #array donde voy a ir guardando los valores de accuracy para cada punto de corte\n",
    "recall = [] #ídem para recall\n",
    "prec = [] #ídem para precision\n",
    "f1 = [] #íðem para f1 score\n",
    "\n",
    "\n",
    "for punto in puntos_corte: #para cada punto de corte\n",
    "\n",
    "  # calculo las predicciones para ese punto de corte\n",
    "  y_pred_test_custom = np.ones(y_test.shape)\n",
    "  for i in range(probas.shape[0]):\n",
    "      if (probas[i,0]>punto):\n",
    "          y_pred_test_custom[i]=0.\n",
    "  \n",
    "  # calculo las métricas para ese punto de corte y las guardo los arrays antes creados\n",
    "  acc.append( accuracy_score(y_test, y_pred_test_custom) ) #.append agrega al final del array\n",
    "  recall.append( recall_score(y_test, y_pred_test_custom) )\n",
    "  prec.append( precision_score(y_test, y_pred_test_custom) )\n",
    "  f1.append( f1_score(y_test, y_pred_test_custom) )\n",
    "\n",
    "\n",
    "# LO QUE SIGUE ES UN GRÁFICO de los valores guardados en los arrays: acc, recall, prec y f1.\n",
    "# SE PUEDE trabajar directamente con los arrays para sacar más información.\n",
    "\n",
    "plt.plot(puntos_corte, acc, puntos_corte, recall, puntos_corte, prec, puntos_corte, f1)\n",
    "# plt.xlim(0,1)\n",
    "# plt.ylim(0.1)\n",
    "plt.xlabel('Punto de corte')\n",
    "plt.yticks(np.arange(0,1.05,0.05))\n",
    "plt.xticks(np.arange(0,1.05,0.05))\n",
    "plt.grid()\n",
    "plt.axvline(x=0.95, label='Punto de corte', ls=':', lw=2, c='gray')\n",
    "plt.legend(['Accuracy','Recall','Precision','F1 score','Punto de corte']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4ebf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = lg.predict_proba(X_test) #probabilidades de cada clase según modelo predictivo\n",
    "# inspecciono esas probabilidades\n",
    "with np.printoptions(precision=3, suppress=True):\n",
    "    print(probas[0:10])\n",
    "\n",
    "# por ejemplo, por defecto asumo que no falla (0)\n",
    "y_pred_custom = np.zeros(y_test.shape)\n",
    "\n",
    "#y si la probabilidad de y=1 es mayor a 0.85, lo clasifico como falla (1)\n",
    "for i in range(probas.shape[0]):\n",
    "    if (probas[i,0]<0.95):\n",
    "        y_pred_custom[i]=1.\n",
    "\n",
    "\n",
    "conf_custom = confusion_matrix(y_test,y_pred_custom)\n",
    "\n",
    "# grafico matriz de confusión\n",
    "labels = ['VN','FP','FN','VP']\n",
    "categories = ['No Falla', 'Falla']\n",
    "make_confusion_matriz(conf_custom, \n",
    "                     group_names=labels,\n",
    "                     categories=categories, \n",
    "                     cmap='magma',\n",
    "                     cbar=False,\n",
    "                     sum_stats=False,\n",
    "                     figsize=(6,6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d426b6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizamos con Bayes Ingenuo\n",
    "# instanciamos el modelo Bayes ingenuo\n",
    "gnb = GaussianNB() \n",
    "\n",
    "# ajustamos nuestros features y target\n",
    "modelo_gnb = gnb.fit(X_train, y_train) \n",
    "\n",
    "# Ahora hacemos que prediga con nuestros features\n",
    "y_nb_pred = modelo_gnb.predict(X_test) \n",
    "\n",
    "# matriz de confusión\n",
    "conf_nb = confusion_matrix(y_test,y_nb_pred)\n",
    "\n",
    "# grafico matriz de confusión\n",
    "labels = ['VN','FP','FN','VP']\n",
    "categories = ['No Falla', 'Falla']\n",
    "make_confusion_matriz(conf_nb, \n",
    "                     group_names=labels,\n",
    "                     categories=categories, \n",
    "                     cmap='magma',\n",
    "                     cbar=False,\n",
    "                     figsize=(6,6))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daca74cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funciona solamente para clasificación binaria (2 clases)\n",
    "# para muchas clases se puede ver esto: \n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py\n",
    "plot_roc_curve(modelo_gnb, X_test, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01096974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predigo probabilidades según el modelo con los datos de TESTEO.\n",
    "# Luego uso estas probabilidades para, mediante punto de corte, clasificar en clases.\n",
    "probas = modelo_gnb.predict_proba(X_test)\n",
    "\n",
    "# array con los puntos de corte\n",
    "puntos_corte = np.arange(0.02, 1, 0.02) # empieza, termina, paso\n",
    "acc = [] #array donde voy a ir guardando los valores de accuracy para cada punto de corte\n",
    "recall = [] #ídem para recall\n",
    "prec = [] #ídem para precision\n",
    "f1 = [] #íðem para f1 score\n",
    "\n",
    "\n",
    "for punto in puntos_corte: #para cada punto de corte\n",
    "\n",
    "  # calculo las predicciones para ese punto de corte\n",
    "  y_pred_test_custom = np.ones(y_test.shape)\n",
    "  for i in range(probas.shape[0]):\n",
    "      if (probas[i,0]>punto):\n",
    "          y_pred_test_custom[i]=0.\n",
    "  \n",
    "  # calculo las métricas para ese punto de corte y las guardo los arrays antes creados\n",
    "  acc.append( accuracy_score(y_test, y_pred_test_custom) ) #.append agrega al final del array\n",
    "  recall.append( recall_score(y_test, y_pred_test_custom) )\n",
    "  prec.append( precision_score(y_test, y_pred_test_custom) )\n",
    "  f1.append( f1_score(y_test, y_pred_test_custom) )\n",
    "\n",
    "\n",
    "# LO QUE SIGUE ES UN GRÁFICO de los valores guardados en los arrays: acc, recall, prec y f1.\n",
    "# SE PUEDE trabajar directamente con los arrays para sacar más información.\n",
    "\n",
    "plt.plot(puntos_corte, acc, puntos_corte, recall, puntos_corte, prec, puntos_corte, f1)\n",
    "# plt.xlim(0,1)\n",
    "# plt.ylim(0.1)\n",
    "plt.xlabel('Punto de corte')\n",
    "plt.yticks(np.arange(0,1.05,0.05))\n",
    "plt.xticks(np.arange(0,1.05,0.05))\n",
    "plt.grid()\n",
    "plt.axvline(x=0.875, label='Punto de corte', ls=':', lw=2, c='gray')\n",
    "plt.legend(['Accuracy','Recall','Precision','F1 score','Punto de corte']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266480b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = gnb.predict_proba(X_test) #probabilidades de cada clase según modelo predictivo\n",
    "# inspecciono esas probabilidades\n",
    "with np.printoptions(precision=3, suppress=True):\n",
    "    print(probas[0:10])\n",
    "\n",
    "# por ejemplo, por defecto asumo que no falla (0)\n",
    "y_pred_custom = np.zeros(y_test.shape)\n",
    "\n",
    "#y si la probabilidad de y=1 es mayor a 0.85, lo clasifico como falla (1)\n",
    "for i in range(probas.shape[0]):\n",
    "    if (probas[i,0]<0.875):\n",
    "        y_pred_custom[i]=1.\n",
    "\n",
    "\n",
    "conf_custom = confusion_matrix(y_test,y_pred_custom)\n",
    "\n",
    "# grafico matriz de confusión\n",
    "labels = ['VN','FP','FN','VP']\n",
    "categories = ['No Falla', 'Falla']\n",
    "make_confusion_matriz(conf_custom, \n",
    "                     group_names=labels,\n",
    "                     categories=categories, \n",
    "                     cmap='magma',\n",
    "                     cbar=False,\n",
    "                     sum_stats=False,\n",
    "                     figsize=(6,6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0429881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizamos con Arboles de decision\n",
    "# instanciamos el modelo \n",
    "# los parametros se ajustaron con prueba y error\n",
    "dtc = tree.DecisionTreeClassifier(criterion='gini',\n",
    "                                             min_samples_split=20,\n",
    "                                             min_samples_leaf=5,\n",
    "                                             max_depth = 5,\n",
    "                                             class_weight={1:25})\n",
    "\n",
    "# ajustamos nuestros features y target\n",
    "modelo_dtc = dtc.fit(X_train, y_train) \n",
    "\n",
    "# Ahora hacemos que prediga con nuestros features\n",
    "y_dt_pred = modelo_dtc.predict(X_test) \n",
    "\n",
    "# matriz de confusión\n",
    "conf_dt = confusion_matrix(y_test,y_dt_pred)\n",
    "\n",
    "# grafico matriz de confusión\n",
    "labels = ['VN','FP','FN','VP']\n",
    "categories = ['No Falla', 'Falla']\n",
    "make_confusion_matriz(conf_dt, \n",
    "                     group_names=labels,\n",
    "                     categories=categories, \n",
    "                     cmap='magma',\n",
    "                     cbar=False,\n",
    "                     sum_stats=False,\n",
    "                     figsize=(6,6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1fb85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predigo probabilidades según el modelo con los datos de TESTEO.\n",
    "# Luego uso estas probabilidades para, mediante punto de corte, clasificar en clases.\n",
    "probas = modelo_dtc.predict_proba(X_test)\n",
    "\n",
    "# array con los puntos de corte\n",
    "puntos_corte = np.arange(0.02, 1, 0.02) # empieza, termina, paso\n",
    "acc = [] #array donde voy a ir guardando los valores de accuracy para cada punto de corte\n",
    "recall = [] #ídem para recall\n",
    "prec = [] #ídem para precision\n",
    "f1 = [] #íðem para f1 score\n",
    "\n",
    "\n",
    "for punto in puntos_corte: #para cada punto de corte\n",
    "\n",
    "  # calculo las predicciones para ese punto de corte\n",
    "  y_pred_test_custom = np.ones(y_test.shape)\n",
    "  for i in range(probas.shape[0]):\n",
    "      if (probas[i,0]>punto):\n",
    "          y_pred_test_custom[i]=0.\n",
    "  \n",
    "  # calculo las métricas para ese punto de corte y las guardo los arrays antes creados\n",
    "  acc.append( accuracy_score(y_test, y_pred_test_custom) ) #.append agrega al final del array\n",
    "  recall.append( recall_score(y_test, y_pred_test_custom) )\n",
    "  prec.append( precision_score(y_test, y_pred_test_custom) )\n",
    "  f1.append( f1_score(y_test, y_pred_test_custom) )\n",
    "\n",
    "\n",
    "# LO QUE SIGUE ES UN GRÁFICO de los valores guardados en los arrays: acc, recall, prec y f1.\n",
    "# SE PUEDE trabajar directamente con los arrays para sacar más información.\n",
    "\n",
    "plt.plot(puntos_corte, acc, puntos_corte, recall, puntos_corte, prec, puntos_corte, f1)\n",
    "# plt.xlim(0,1)\n",
    "# plt.ylim(0.1)\n",
    "plt.xlabel('Punto de corte')\n",
    "plt.yticks(np.arange(0,1.05,0.05))\n",
    "plt.xticks(np.arange(0,1.05,0.05))\n",
    "plt.grid()\n",
    "plt.axvline(x=0.5, label='Punto de corte', ls=':', lw=2, c='gray')\n",
    "plt.legend(['Accuracy','Recall','Precision','F1 score','Punto de corte']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf7065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = dtc.predict_proba(X_test) #probabilidades de cada clase según modelo predictivo\n",
    "# inspecciono esas probabilidades\n",
    "with np.printoptions(precision=3, suppress=True):\n",
    "    print(probas[0:10])\n",
    "\n",
    "# por ejemplo, por defecto asumo que no falla (0)\n",
    "y_pred_custom = np.zeros(y_test.shape)\n",
    "\n",
    "#y si la probabilidad de y=1 es mayor a 0.85, lo clasifico como falla (1)\n",
    "for i in range(probas.shape[0]):\n",
    "    if (probas[i,0]<0.4):\n",
    "        y_pred_custom[i]=1.\n",
    "\n",
    "\n",
    "conf_custom = confusion_matrix(y_test,y_pred_custom)\n",
    "\n",
    "# grafico matriz de confusión\n",
    "labels = ['VN','FP','FN','VP']\n",
    "categories = ['No Falla', 'Falla']\n",
    "make_confusion_matriz(conf_custom, \n",
    "                     group_names=labels,\n",
    "                     categories=categories, \n",
    "                     cmap='magma',\n",
    "                     cbar=False,\n",
    "                     sum_stats=False, \n",
    "                     figsize=(6,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd3fd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funciona solamente para clasificación binaria (2 clases)\n",
    "# para muchas clases se puede ver esto: \n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py\n",
    "plot_roc_curve(modelo_dtc, X_test, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747129e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos PCA\n",
    "#Componentes principales\n",
    "pca = PCA()\n",
    "pca.fit(X_normal,y_tg1)\n",
    "x_new = pca.transform(X_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7143574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instancio la clase\n",
    "pca = PCA(n_components=2)\n",
    "# calculo PCA\n",
    "pca.fit(X_normal)\n",
    "# transformo los datos originales al plano PCA\n",
    "pca_data = pca.transform(X_normal)\n",
    "\n",
    "# cálculo varianza explicada\n",
    "vars = pca.explained_variance_ratio_\n",
    "var1 = round(100*vars[0],2)\n",
    "var2 = round(100*vars[1],2)\n",
    "\n",
    "# gráfico\n",
    "plt.scatter(x=pca_data[:,0], y=pca_data[:,1], lw=2)\n",
    "plt.xlabel(\"Componente principal 1 ({}%)\".format(var1))\n",
    "plt.ylabel(\"Componente principal 2 ({}%)\".format(var2))\n",
    "plt.title(\"Análisis por componentes principales\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c146b7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_scores = []\n",
    "rango_clusters = range(2,6)\n",
    "\n",
    "for i in rango_clusters:\n",
    "    # para cada valor de i, calculo kmeans y silhouette\n",
    "    k_means = KMeans(n_clusters=i,random_state=100000, n_init=20)\n",
    "    #k_means = KMeans(n_clusters=i)\n",
    "    k_means.fit(X_normal)\n",
    "    \n",
    "    sil_score = silhouette_score(X_normal, labels=k_means.labels_)\n",
    "    silhouette_scores.append(sil_score)\n",
    "\n",
    "plt.plot(rango_clusters,silhouette_scores)\n",
    "plt.xticks(rango_clusters)\n",
    "plt.xlabel('Cantidad de clusters')\n",
    "plt.ylabel('Score Silhouette')\n",
    "plt.title('Los máximos locales son los candidatos según Silhouette')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e654268c",
   "metadata": {},
   "source": [
    "Clasificacion no Supervisada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd42385e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elijo la cantidad de clusters\n",
    "cant_clusters = 2\n",
    "# instancio la clase\n",
    "kmeans = KMeans(n_clusters=cant_clusters,random_state=100000, n_init=10)\n",
    "# entreno al modelo\n",
    "kmeans.fit(X_normal)\n",
    "# que me prediga el cluster para cada dato\n",
    "y_kmeans = pd.Series(kmeans.predict(X_normal))\n",
    "\n",
    "# que me diga los centroides\n",
    "centroides = kmeans.cluster_centers_\n",
    "# transformo los centroides al plano PCA para poderlos graficar\n",
    "centroides_pca = pca.transform(centroides)\n",
    "\n",
    "# grafico en el plano PCA datos, clusters y centroides\n",
    "plt.scatter(pca_data[:,0],pca_data[:,1], c=kmeans.labels_, cmap=\"Paired\")\n",
    "#plt.scatter(centroides_pca[:,0],centroides_pca[:, 1], c=\"red\", s=200)\n",
    "plt.title(\"Visualización de clusters por PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023da8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a ver como se agrupan los cluster sacando su media y desviacion\n",
    "df_class_kmeans = pd.Series(kmeans.fit_predict(df))\n",
    "\n",
    "# Agrega una nueva variable con el cluster al que pertenece cada observación\n",
    "df[\"Cluster\"] = df_class_kmeans\n",
    "\n",
    "# Calculo la media de cada variable para cada cluster\n",
    "df.groupby([\"Cluster\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489c9811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculo la desviacion de cada variable para cada cluster\n",
    "df.groupby([\"Cluster\"]).std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfa31be",
   "metadata": {},
   "source": [
    "Ahora con Aglomerativo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247abdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pop('Cluster')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cbea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "politica_agrupamiento = ['ward', 'complete', 'average', 'single']\n",
    "cant_cluster = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b644b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = AgglomerativeClustering(n_clusters=cant_cluster, linkage=politica_agrupamiento[0])\n",
    "\n",
    "clustering.fit(X_normal)\n",
    "\n",
    "y_jerarq = pd.Series(clustering.labels_)\n",
    "\n",
    "# grafico en el plano PCA datos y clusters\n",
    "plt.scatter(pca_data[:,0],pca_data[:,1], c=clustering.labels_, cmap=\"Paired\")\n",
    "plt.title(\"Visualización de clusters por PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa9754a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrega una nueva variable con el cluster al que pertenece cada observación\n",
    "df[\"Cluster\"] = y_jerarq\n",
    "\n",
    "# Calculo la media de cada variable para cada cluster\n",
    "df.groupby([\"Cluster\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207cb800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculo la desviacion de cada variable para cada cluster\n",
    "df.groupby([\"Cluster\"]).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bae0be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pop('Cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8969b11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = AgglomerativeClustering(n_clusters=cant_cluster, linkage=politica_agrupamiento[1])\n",
    "\n",
    "clustering.fit(X_normal)\n",
    "\n",
    "y_jerarq = pd.Series(clustering.labels_)\n",
    "\n",
    "# grafico en el plano PCA datos y clusters\n",
    "plt.scatter(pca_data[:,0],pca_data[:,1], c=clustering.labels_, cmap=\"Paired\")\n",
    "plt.title(\"Visualización de clusters por PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada92ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrega una nueva variable con el cluster al que pertenece cada observación\n",
    "df[\"Cluster\"] = y_jerarq\n",
    "\n",
    "# Calculo la media de cada variable para cada cluster\n",
    "df.groupby([\"Cluster\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6214b537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculo la desviacion de cada variable para cada cluster\n",
    "df.groupby([\"Cluster\"]).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98e3b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pop('Cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc46e8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = AgglomerativeClustering(n_clusters=cant_cluster, linkage=politica_agrupamiento[2])\n",
    "\n",
    "clustering.fit(X_normal)\n",
    "\n",
    "y_jerarq = pd.Series(clustering.labels_)\n",
    "\n",
    "# grafico en el plano PCA datos y clusters\n",
    "plt.scatter(pca_data[:,0],pca_data[:,1], c=clustering.labels_, cmap=\"Paired\")\n",
    "plt.title(\"Visualización de clusters por PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f620f14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrega una nueva variable con el cluster al que pertenece cada observación\n",
    "df[\"Cluster\"] = y_jerarq\n",
    "\n",
    "# Calculo la media de cada variable para cada cluster\n",
    "df.groupby([\"Cluster\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332f4a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculo la desviacion de cada variable para cada cluster\n",
    "df.groupby([\"Cluster\"]).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30ed0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pop('Cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eded48",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = AgglomerativeClustering(n_clusters=cant_cluster, linkage=politica_agrupamiento[3])\n",
    "\n",
    "clustering.fit(X_normal)\n",
    "\n",
    "y_jerarq = pd.Series(clustering.labels_)\n",
    "\n",
    "# grafico en el plano PCA datos y clusters\n",
    "plt.scatter(pca_data[:,0],pca_data[:,1], c=clustering.labels_, cmap=\"Paired\")\n",
    "plt.title(\"Visualización de clusters por PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8857c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrega una nueva variable con el cluster al que pertenece cada observación\n",
    "df[\"Cluster\"] = y_jerarq\n",
    "\n",
    "# Calculo la media de cada variable para cada cluster\n",
    "df.groupby([\"Cluster\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2e8b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculo la desviacion de cada variable para cada cluster\n",
    "df.groupby([\"Cluster\"]).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea17541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pop('Cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5833509f",
   "metadata": {},
   "source": [
    "Lo anterior se podria haber resuelto con una funcion que tome como parametro el linkage pero por las limitaciones de graficacion de notebbok se decidio copiar y pegar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa394eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": false,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": false,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
