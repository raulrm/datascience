{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5900915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn import model_selection\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "from sklearn.metrics import f1_score, classification_report, plot_roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn import tree\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "sns.set(rc={'figure.figsize':(11,10)})\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f4f492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion auxiliar\n",
    "def make_confusion_matriz(cf,\n",
    "                          group_names=None,\n",
    "                          categories='auto',\n",
    "                          count=True,\n",
    "                          percent=True,\n",
    "                          cbar=True,\n",
    "                          xyticks=True,\n",
    "                          xyplotlabels=True,\n",
    "                          sum_stats=True,\n",
    "                          figsize=None,\n",
    "                          cmap='Blues',\n",
    "                          title=None):\n",
    "    '''\n",
    "    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cf:            confusion matrix to be passed in\n",
    "\n",
    "    group_names:   List of strings that represent the labels row by row to be shown in each square.\n",
    "\n",
    "    categories:    List of strings containing the categories to be displayed on the x,y axis. Default is 'auto'\n",
    "\n",
    "    count:         If True, show the raw number in the confusion matrix. Default is True.\n",
    "\n",
    "    normalize:     If True, show the proportions for each category. Default is True.\n",
    "\n",
    "    cbar:          If True, show the color bar. The cbar values are based off the values in the confusion matrix.\n",
    "                   Default is True.\n",
    "\n",
    "    xyticks:       If True, show x and y ticks. Default is True.\n",
    "\n",
    "    xyplotlabels:  If True, show 'True Label' and 'Predicted Label' on the figure. Default is True.\n",
    "\n",
    "    sum_stats:     If True, display summary statistics below the figure. Default is True.\n",
    "\n",
    "    figsize:       Tuple representing the figure size. Default will be the matplotlib rcParams value.\n",
    "\n",
    "    cmap:          Colormap of the values displayed from matplotlib.pyplot.cm. Default is 'Blues'\n",
    "                   See http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                   \n",
    "    title:         Title for the heatmap. Default is None.\n",
    "\n",
    "    '''\n",
    "\n",
    "\n",
    "    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n",
    "    blanks = ['' for i in range(cf.size)]\n",
    "\n",
    "    if group_names and len(group_names)==cf.size:\n",
    "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
    "    else:\n",
    "        group_labels = blanks\n",
    "\n",
    "    if count:\n",
    "        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n",
    "    else:\n",
    "        group_counts = blanks\n",
    "\n",
    "    if percent:\n",
    "        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()/np.sum(cf)]\n",
    "    else:\n",
    "        group_percentages = blanks\n",
    "\n",
    "    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n",
    "    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n",
    "\n",
    "\n",
    "    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n",
    "    if sum_stats:\n",
    "        #Accuracy is sum of diagonal divided by total observations\n",
    "        accuracy  = np.trace(cf) / float(np.sum(cf))\n",
    "\n",
    "        #if it is a binary confusion matrix, show some more stats\n",
    "        if len(cf)==2:\n",
    "            #Metrics for Binary Confusion Matrices\n",
    "            precision = cf[1,1] / sum(cf[:,1])\n",
    "            recall    = cf[1,1] / sum(cf[1,:])\n",
    "            f1_score  = 2*precision*recall / (precision + recall)\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n",
    "                accuracy,precision,recall,f1_score)\n",
    "        else:\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n",
    "    else:\n",
    "        stats_text = \"\"\n",
    "\n",
    "\n",
    "    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n",
    "    if figsize==None:\n",
    "        #Get default figure size if not set\n",
    "        figsize = plt.rcParams.get('figure.figsize')\n",
    "\n",
    "    if xyticks==False:\n",
    "        #Do not show categories if xyticks is False\n",
    "        categories=False\n",
    "\n",
    "\n",
    "    # MAKE THE HEATMAP VISUALIZATION\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cf,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n",
    "\n",
    "    if xyplotlabels:\n",
    "        plt.ylabel('Valores Reales')\n",
    "        plt.xlabel('Valores Predichos' + stats_text)\n",
    "    else:\n",
    "        plt.xlabel(stats_text)\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbef872",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"mantenimiento.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfc7851",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27745a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8422b3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f00349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necesitamos hacer\n",
    "# 1 Descartar UDI y Product ID\n",
    "# 2 Hacer numerica Type L=0, M=1, H=2\n",
    "# 3 Hacer numerica Failure Type\n",
    "# 4 Separar Features y Target como target1 y Failure Type como target2\n",
    "# 5 Escalar el resto como features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09be5937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos los valores distintos de Failure Type\n",
    "df['Failure Type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bddbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos los valores distintos de Failure Type\n",
    "df['Type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feb740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos los valores distintos de Failure Type\n",
    "df['Target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c31acb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Descartar UDI y Product ID\n",
    "df.drop(df.columns[:2],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6a6371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Hacer numerica Type L=0, M=1, H=2\n",
    "# 3 Hacer numerica Failure Type\n",
    "\n",
    "dict_reemplazo = {\"Type\":     {\"L\": 0, \"M\": 1, 'H': 2},\n",
    "                   \"Failure Type\":     {'No Failure' : 0,\n",
    "                                        'Power Failure' : 1,\n",
    "                                        'Tool Wear Failure' : 2,\n",
    "                                        'Overstrain Failure' : 3,\n",
    "                                        'Random Failures' : 4,\n",
    "                                        'Heat Dissipation Failure' : 5}\n",
    "               }\n",
    "# Aplicamos el reemplazo en el dataframe\n",
    "df = df.replace(dict_reemplazo)\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81b1928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos los valores distintos de Failure Type\n",
    "df['Type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46da26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6cba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 Separar Features y Target como target1 y Failure Type como target2\n",
    "X = df.iloc[:,0:-2]\n",
    "y_tg1 = df.iloc[:,-2:-1]\n",
    "y_tg2 = df.iloc[:,-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd0f9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.pairplot(df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a0096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Target', data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32da3715",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Failure Type', data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b441760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contamos los valores distintos\n",
    "df['Target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c61ab2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contamos los valores distintos\n",
    "df['Failure Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b9bda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d39c10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1341b6",
   "metadata": {},
   "source": [
    "Vamos a analizar con falla/no falla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0229ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para escalar \n",
    "# Normalizamos todo el dataset\n",
    "X_normal=(X - X.mean()) / X.std()\n",
    "X_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790c7533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos PCA\n",
    "#Componentes principales\n",
    "pca = PCA()\n",
    "pca.fit(X_normal,y_tg1)\n",
    "x_new = pca.transform(X_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38ee0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Varianza explicada por cada componente\n",
    "var_pc = pca.explained_variance_ratio_\n",
    "#print(var_pc)\n",
    "\n",
    "indice = 1\n",
    "for var in var_pc:\n",
    "    print(\"PCA{} : {}\".format(indice, round(var,4)))\n",
    "    indice += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d897605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos los nombres de las columnas\n",
    "nombres = list(X_normal.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692d392a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colors\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "green_patch = mpatches.Patch(color='green', label='No Falla')\n",
    "red_patch = mpatches.Patch(color='red', label='Falla')\n",
    "\n",
    "\n",
    "# asignamos los dos primeros PC a los ejes coordinados\n",
    "x_valor = x_new[:,0]\n",
    "y_valor = x_new[:,1]\n",
    "\n",
    "cmap = colors.ListedColormap(['green', 'red'])\n",
    "\n",
    "dot = list(y_tg1.Target.values)\n",
    "\n",
    "# hacemos un grafico de dispersion\n",
    "dispersion_1 = plt.scatter(x_valor, y_valor , c=dot, cmap=cmap)\n",
    "\n",
    "# Le ponemos nombre a los ejes\n",
    "plt.xlabel(\"PC1\");\n",
    "plt.ylabel(\"PC2\");\n",
    "\n",
    "# mostramos la leyenda\n",
    "plt.legend(handles=[green_patch, red_patch], fontsize=12)\n",
    "\n",
    "# Mostamos la grilla\n",
    "plt.grid();\n",
    "\n",
    "# imprimimos el grafico completo\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1e89cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el rango de los ejes del grafico\n",
    "plt.axis([-0.4,1,-0.75,0.75])\n",
    "\n",
    "# Vemos cuantos vectores son las direcciones de maxima varianza\n",
    "n = pca.components_.shape[0]\n",
    "\n",
    "# Recorremos esos vectores y los vamos dibujando en el plano\n",
    "for i in range(n):\n",
    "    plt.arrow(0, 0, pca.components_[i,0], pca.components_[i,1], color = 'r', alpha = 1);\n",
    "    # En el extremo de cada vector ponemos en nombre de la columan correspondiente (un poco dezplazados)\n",
    "    plt.text(pca.components_[i,0]*1.1 , pca.components_[i,1]*1.1, nombres[i], color = 'g', ha = 'center', va = 'center', fontsize=12);\n",
    "\n",
    "# Le ponemos nombre a los ejes\n",
    "plt.xlabel(\"PC1\");\n",
    "plt.ylabel(\"PC2\");\n",
    "\n",
    "# imprimimos el grafico completo\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c64137",
   "metadata": {},
   "source": [
    "Clasificacion Supervisada\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c6615d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normal, y_tg1, stratify=y_tg1, test_size=0.2, random_state=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54518123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probamos con regresion logistica\n",
    "lg = LogisticRegression() \n",
    "modelo_lg = lg.fit(X_train, y_train) \n",
    "y_pred_test = modelo_lg.predict(X_test) \n",
    "\n",
    "# matriz de confusión\n",
    "conf = confusion_matrix(y_test,y_pred_test)\n",
    "\n",
    "# grafico matriz de confusión\n",
    "labels = ['VN','FP','FN','VP']\n",
    "categories = ['No Falla', 'Falla']\n",
    "make_confusion_matriz(conf, \n",
    "                     group_names=labels,\n",
    "                     categories=categories, \n",
    "                     cmap='magma',\n",
    "                     cbar=False,\n",
    "                     sum_stats=False,\n",
    "                     figsize=(6,6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0654a211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funciona solamente para clasificación binaria (2 clases)\n",
    "# para muchas clases se puede ver esto: \n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py\n",
    "plot_roc_curve(modelo_lg, X_test, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3bdb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predigo probabilidades según el modelo con los datos de TESTEO.\n",
    "# Luego uso estas probabilidades para, mediante punto de corte, clasificar en clases.\n",
    "probas = modelo_lg.predict_proba(X_test)\n",
    "\n",
    "# array con los puntos de corte\n",
    "puntos_corte = np.arange(0.02, 1, 0.02) # empieza, termina, paso\n",
    "acc = [] #array donde voy a ir guardando los valores de accuracy para cada punto de corte\n",
    "recall = [] #ídem para recall\n",
    "prec = [] #ídem para precision\n",
    "f1 = [] #íðem para f1 score\n",
    "\n",
    "\n",
    "for punto in puntos_corte: #para cada punto de corte\n",
    "\n",
    "  # calculo las predicciones para ese punto de corte\n",
    "  y_pred_test_custom = np.ones(y_test.shape)\n",
    "  for i in range(probas.shape[0]):\n",
    "      if (probas[i,0]>punto):\n",
    "          y_pred_test_custom[i]=0.\n",
    "  \n",
    "  # calculo las métricas para ese punto de corte y las guardo los arrays antes creados\n",
    "  acc.append( accuracy_score(y_test, y_pred_test_custom) ) #.append agrega al final del array\n",
    "  recall.append( recall_score(y_test, y_pred_test_custom) )\n",
    "  prec.append( precision_score(y_test, y_pred_test_custom) )\n",
    "  f1.append( f1_score(y_test, y_pred_test_custom) )\n",
    "\n",
    "\n",
    "# LO QUE SIGUE ES UN GRÁFICO de los valores guardados en los arrays: acc, recall, prec y f1.\n",
    "# SE PUEDE trabajar directamente con los arrays para sacar más información.\n",
    "\n",
    "plt.plot(puntos_corte, acc, puntos_corte, recall, puntos_corte, prec, puntos_corte, f1)\n",
    "# plt.xlim(0,1)\n",
    "# plt.ylim(0.1)\n",
    "plt.xlabel('Punto de corte')\n",
    "plt.yticks(np.arange(0,1.05,0.05))\n",
    "plt.xticks(np.arange(0,1.05,0.05))\n",
    "plt.grid()\n",
    "plt.axvline(x=0.95, label='Punto de corte', ls=':', lw=2, c='gray')\n",
    "plt.legend(['Accuracy','Recall','Precision','F1 score','Punto de corte']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2849b651",
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = lg.predict_proba(X_test) #probabilidades de cada clase según modelo predictivo\n",
    "# inspecciono esas probabilidades\n",
    "with np.printoptions(precision=3, suppress=True):\n",
    "    print(probas[0:10])\n",
    "\n",
    "# por ejemplo, por defecto asumo que no falla (0)\n",
    "y_pred_custom = np.zeros(y_test.shape)\n",
    "\n",
    "#y si la probabilidad de y=1 es mayor a 0.85, lo clasifico como falla (1)\n",
    "for i in range(probas.shape[0]):\n",
    "    if (probas[i,0]<0.95):\n",
    "        y_pred_custom[i]=1.\n",
    "\n",
    "\n",
    "conf_custom = confusion_matrix(y_test,y_pred_custom)\n",
    "\n",
    "# grafico matriz de confusión\n",
    "labels = ['VN','FP','FN','VP']\n",
    "categories = ['No Falla', 'Falla']\n",
    "make_confusion_matriz(conf_custom, \n",
    "                     group_names=labels,\n",
    "                     categories=categories, \n",
    "                     cmap='magma',\n",
    "                     cbar=False,\n",
    "                     sum_stats=False,\n",
    "                     figsize=(6,6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a547f8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizamos con Bayes Ingenuo\n",
    "# instanciamos el modelo Bayes ingenuo\n",
    "gnb = GaussianNB() \n",
    "\n",
    "# ajustamos nuestros features y target\n",
    "modelo_gnb = gnb.fit(X_train, y_train) \n",
    "\n",
    "# Ahora hacemos que prediga con nuestros features\n",
    "y_nb_pred = modelo_gnb.predict(X_test) \n",
    "\n",
    "# matriz de confusión\n",
    "conf_nb = confusion_matrix(y_test,y_nb_pred)\n",
    "\n",
    "# grafico matriz de confusión\n",
    "labels = ['VN','FP','FN','VP']\n",
    "categories = ['No Falla', 'Falla']\n",
    "make_confusion_matriz(conf_nb, \n",
    "                     group_names=labels,\n",
    "                     categories=categories, \n",
    "                     cmap='magma',\n",
    "                     cbar=False,\n",
    "                     figsize=(6,6))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc98b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funciona solamente para clasificación binaria (2 clases)\n",
    "# para muchas clases se puede ver esto: \n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py\n",
    "plot_roc_curve(modelo_gnb, X_test, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936c8852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predigo probabilidades según el modelo con los datos de TESTEO.\n",
    "# Luego uso estas probabilidades para, mediante punto de corte, clasificar en clases.\n",
    "probas = modelo_gnb.predict_proba(X_test)\n",
    "\n",
    "# array con los puntos de corte\n",
    "puntos_corte = np.arange(0.02, 1, 0.02) # empieza, termina, paso\n",
    "acc = [] #array donde voy a ir guardando los valores de accuracy para cada punto de corte\n",
    "recall = [] #ídem para recall\n",
    "prec = [] #ídem para precision\n",
    "f1 = [] #íðem para f1 score\n",
    "\n",
    "\n",
    "for punto in puntos_corte: #para cada punto de corte\n",
    "\n",
    "  # calculo las predicciones para ese punto de corte\n",
    "  y_pred_test_custom = np.ones(y_test.shape)\n",
    "  for i in range(probas.shape[0]):\n",
    "      if (probas[i,0]>punto):\n",
    "          y_pred_test_custom[i]=0.\n",
    "  \n",
    "  # calculo las métricas para ese punto de corte y las guardo los arrays antes creados\n",
    "  acc.append( accuracy_score(y_test, y_pred_test_custom) ) #.append agrega al final del array\n",
    "  recall.append( recall_score(y_test, y_pred_test_custom) )\n",
    "  prec.append( precision_score(y_test, y_pred_test_custom) )\n",
    "  f1.append( f1_score(y_test, y_pred_test_custom) )\n",
    "\n",
    "\n",
    "# LO QUE SIGUE ES UN GRÁFICO de los valores guardados en los arrays: acc, recall, prec y f1.\n",
    "# SE PUEDE trabajar directamente con los arrays para sacar más información.\n",
    "\n",
    "plt.plot(puntos_corte, acc, puntos_corte, recall, puntos_corte, prec, puntos_corte, f1)\n",
    "# plt.xlim(0,1)\n",
    "# plt.ylim(0.1)\n",
    "plt.xlabel('Punto de corte')\n",
    "plt.yticks(np.arange(0,1.05,0.05))\n",
    "plt.xticks(np.arange(0,1.05,0.05))\n",
    "plt.grid()\n",
    "plt.axvline(x=0.875, label='Punto de corte', ls=':', lw=2, c='gray')\n",
    "plt.legend(['Accuracy','Recall','Precision','F1 score','Punto de corte']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d90c31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = gnb.predict_proba(X_test) #probabilidades de cada clase según modelo predictivo\n",
    "# inspecciono esas probabilidades\n",
    "with np.printoptions(precision=3, suppress=True):\n",
    "    print(probas[0:10])\n",
    "\n",
    "# por ejemplo, por defecto asumo que no falla (0)\n",
    "y_pred_custom = np.zeros(y_test.shape)\n",
    "\n",
    "#y si la probabilidad de y=1 es mayor a 0.85, lo clasifico como falla (1)\n",
    "for i in range(probas.shape[0]):\n",
    "    if (probas[i,0]<0.875):\n",
    "        y_pred_custom[i]=1.\n",
    "\n",
    "\n",
    "conf_custom = confusion_matrix(y_test,y_pred_custom)\n",
    "\n",
    "# grafico matriz de confusión\n",
    "labels = ['VN','FP','FN','VP']\n",
    "categories = ['No Falla', 'Falla']\n",
    "make_confusion_matriz(conf_custom, \n",
    "                     group_names=labels,\n",
    "                     categories=categories, \n",
    "                     cmap='magma',\n",
    "                     cbar=False,\n",
    "                     sum_stats=False,\n",
    "                     figsize=(6,6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b3bdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizamos con Arboles de decision\n",
    "# instanciamos el modelo \n",
    "# los parametros se ajustaron con prueba y error\n",
    "dtc = tree.DecisionTreeClassifier(criterion='gini',\n",
    "                                             min_samples_split=20,\n",
    "                                             min_samples_leaf=5,\n",
    "                                             max_depth = 5,\n",
    "                                             class_weight={1:25})\n",
    "\n",
    "# ajustamos nuestros features y target\n",
    "modelo_dtc = dtc.fit(X_train, y_train) \n",
    "\n",
    "# Ahora hacemos que prediga con nuestros features\n",
    "y_dt_pred = modelo_dtc.predict(X_test) \n",
    "\n",
    "# matriz de confusión\n",
    "conf_dt = confusion_matrix(y_test,y_dt_pred)\n",
    "\n",
    "# grafico matriz de confusión\n",
    "labels = ['VN','FP','FN','VP']\n",
    "categories = ['No Falla', 'Falla']\n",
    "make_confusion_matriz(conf_dt, \n",
    "                     group_names=labels,\n",
    "                     categories=categories, \n",
    "                     cmap='magma',\n",
    "                     cbar=False,\n",
    "                     sum_stats=False,\n",
    "                     figsize=(6,6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa16b216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predigo probabilidades según el modelo con los datos de TESTEO.\n",
    "# Luego uso estas probabilidades para, mediante punto de corte, clasificar en clases.\n",
    "probas = modelo_dtc.predict_proba(X_test)\n",
    "\n",
    "# array con los puntos de corte\n",
    "puntos_corte = np.arange(0.02, 1, 0.02) # empieza, termina, paso\n",
    "acc = [] #array donde voy a ir guardando los valores de accuracy para cada punto de corte\n",
    "recall = [] #ídem para recall\n",
    "prec = [] #ídem para precision\n",
    "f1 = [] #íðem para f1 score\n",
    "\n",
    "\n",
    "for punto in puntos_corte: #para cada punto de corte\n",
    "\n",
    "  # calculo las predicciones para ese punto de corte\n",
    "  y_pred_test_custom = np.ones(y_test.shape)\n",
    "  for i in range(probas.shape[0]):\n",
    "      if (probas[i,0]>punto):\n",
    "          y_pred_test_custom[i]=0.\n",
    "  \n",
    "  # calculo las métricas para ese punto de corte y las guardo los arrays antes creados\n",
    "  acc.append( accuracy_score(y_test, y_pred_test_custom) ) #.append agrega al final del array\n",
    "  recall.append( recall_score(y_test, y_pred_test_custom) )\n",
    "  prec.append( precision_score(y_test, y_pred_test_custom) )\n",
    "  f1.append( f1_score(y_test, y_pred_test_custom) )\n",
    "\n",
    "\n",
    "# LO QUE SIGUE ES UN GRÁFICO de los valores guardados en los arrays: acc, recall, prec y f1.\n",
    "# SE PUEDE trabajar directamente con los arrays para sacar más información.\n",
    "\n",
    "plt.plot(puntos_corte, acc, puntos_corte, recall, puntos_corte, prec, puntos_corte, f1)\n",
    "# plt.xlim(0,1)\n",
    "# plt.ylim(0.1)\n",
    "plt.xlabel('Punto de corte')\n",
    "plt.yticks(np.arange(0,1.05,0.05))\n",
    "plt.xticks(np.arange(0,1.05,0.05))\n",
    "plt.grid()\n",
    "plt.axvline(x=0.5, label='Punto de corte', ls=':', lw=2, c='gray')\n",
    "plt.legend(['Accuracy','Recall','Precision','F1 score','Punto de corte']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c4056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = dtc.predict_proba(X_test) #probabilidades de cada clase según modelo predictivo\n",
    "# inspecciono esas probabilidades\n",
    "with np.printoptions(precision=3, suppress=True):\n",
    "    print(probas[0:10])\n",
    "\n",
    "# por ejemplo, por defecto asumo que no falla (0)\n",
    "y_pred_custom = np.zeros(y_test.shape)\n",
    "\n",
    "#y si la probabilidad de y=1 es mayor a 0.85, lo clasifico como falla (1)\n",
    "for i in range(probas.shape[0]):\n",
    "    if (probas[i,0]<0.4):\n",
    "        y_pred_custom[i]=1.\n",
    "\n",
    "\n",
    "conf_custom = confusion_matrix(y_test,y_pred_custom)\n",
    "\n",
    "# grafico matriz de confusión\n",
    "labels = ['VN','FP','FN','VP']\n",
    "categories = ['No Falla', 'Falla']\n",
    "make_confusion_matriz(conf_custom, \n",
    "                     group_names=labels,\n",
    "                     categories=categories, \n",
    "                     cmap='magma',\n",
    "                     cbar=False,\n",
    "                     sum_stats=False, \n",
    "                     figsize=(6,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dff726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funciona solamente para clasificación binaria (2 clases)\n",
    "# para muchas clases se puede ver esto: \n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py\n",
    "plot_roc_curve(modelo_dtc, X_test, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6098f878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos PCA\n",
    "#Componentes principales\n",
    "pca = PCA()\n",
    "pca.fit(X_normal,y_tg1)\n",
    "x_new = pca.transform(X_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e7ab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instancio la clase\n",
    "pca = PCA(n_components=2)\n",
    "# calculo PCA\n",
    "pca.fit(X_normal)\n",
    "# transformo los datos originales al plano PCA\n",
    "pca_data = pca.transform(X_normal)\n",
    "\n",
    "# cálculo varianza explicada\n",
    "vars = pca.explained_variance_ratio_\n",
    "var1 = round(100*vars[0],2)\n",
    "var2 = round(100*vars[1],2)\n",
    "\n",
    "# gráfico\n",
    "plt.scatter(x=pca_data[:,0], y=pca_data[:,1], lw=2)\n",
    "plt.xlabel(\"Componente principal 1 ({}%)\".format(var1))\n",
    "plt.ylabel(\"Componente principal 2 ({}%)\".format(var2))\n",
    "plt.title(\"Análisis por componentes principales\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a717e31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_scores = []\n",
    "rango_clusters = range(2,6)\n",
    "\n",
    "for i in rango_clusters:\n",
    "    # para cada valor de i, calculo kmeans y silhouette\n",
    "    k_means = KMeans(n_clusters=i,random_state=100000, n_init=20)\n",
    "    #k_means = KMeans(n_clusters=i)\n",
    "    k_means.fit(X_normal)\n",
    "    \n",
    "    sil_score = silhouette_score(X_normal, labels=k_means.labels_)\n",
    "    silhouette_scores.append(sil_score)\n",
    "\n",
    "plt.plot(rango_clusters,silhouette_scores)\n",
    "plt.xticks(rango_clusters)\n",
    "plt.xlabel('Cantidad de clusters')\n",
    "plt.ylabel('Score Silhouette')\n",
    "plt.title('Los máximos locales son los candidatos según Silhouette')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dee247",
   "metadata": {},
   "source": [
    "Clasificacion no Supervisada\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8759c6",
   "metadata": {},
   "source": [
    "Ahora con Aglomerativo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36336f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "politica_agrupamiento = ['ward', 'complete', 'average', 'single']\n",
    "cant_cluster = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a57211e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = AgglomerativeClustering(n_clusters=cant_cluster, linkage=politica_agrupamiento[0])\n",
    "\n",
    "clustering.fit(X_normal)\n",
    "\n",
    "y_jerarq = pd.Series(clustering.labels_)\n",
    "\n",
    "# grafico en el plano PCA datos y clusters\n",
    "plt.scatter(pca_data[:,0],pca_data[:,1], c=clustering.labels_, cmap=\"Paired\")\n",
    "plt.title(\"Visualización de clusters por PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1048cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrega una nueva variable con el cluster al que pertenece cada observación\n",
    "df[\"Cluster\"] = y_jerarq\n",
    "\n",
    "# Calculo la media de cada variable para cada cluster\n",
    "df.groupby([\"Cluster\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9d95f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculo la desviacion de cada variable para cada cluster\n",
    "df.groupby([\"Cluster\"]).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c657457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pop('Cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61932262",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = AgglomerativeClustering(n_clusters=cant_cluster, linkage=politica_agrupamiento[1])\n",
    "\n",
    "clustering.fit(X_normal)\n",
    "\n",
    "y_jerarq = pd.Series(clustering.labels_)\n",
    "\n",
    "# grafico en el plano PCA datos y clusters\n",
    "plt.scatter(pca_data[:,0],pca_data[:,1], c=clustering.labels_, cmap=\"Paired\")\n",
    "plt.title(\"Visualización de clusters por PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19349a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrega una nueva variable con el cluster al que pertenece cada observación\n",
    "df[\"Cluster\"] = y_jerarq\n",
    "\n",
    "# Calculo la media de cada variable para cada cluster\n",
    "df.groupby([\"Cluster\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828db79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculo la desviacion de cada variable para cada cluster\n",
    "df.groupby([\"Cluster\"]).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1893f968",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pop('Cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db324e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = AgglomerativeClustering(n_clusters=cant_cluster, linkage=politica_agrupamiento[2])\n",
    "\n",
    "clustering.fit(X_normal)\n",
    "\n",
    "y_jerarq = pd.Series(clustering.labels_)\n",
    "\n",
    "# grafico en el plano PCA datos y clusters\n",
    "plt.scatter(pca_data[:,0],pca_data[:,1], c=clustering.labels_, cmap=\"Paired\")\n",
    "plt.title(\"Visualización de clusters por PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649520af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrega una nueva variable con el cluster al que pertenece cada observación\n",
    "df[\"Cluster\"] = y_jerarq\n",
    "\n",
    "# Calculo la media de cada variable para cada cluster\n",
    "df.groupby([\"Cluster\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647ac1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculo la desviacion de cada variable para cada cluster\n",
    "df.groupby([\"Cluster\"]).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2659c12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pop('Cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60096352",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = AgglomerativeClustering(n_clusters=cant_cluster, linkage=politica_agrupamiento[3])\n",
    "\n",
    "clustering.fit(X_normal)\n",
    "\n",
    "y_jerarq = pd.Series(clustering.labels_)\n",
    "\n",
    "# grafico en el plano PCA datos y clusters\n",
    "plt.scatter(pca_data[:,0],pca_data[:,1], c=clustering.labels_, cmap=\"Paired\")\n",
    "plt.title(\"Visualización de clusters por PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586e4bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrega una nueva variable con el cluster al que pertenece cada observación\n",
    "df[\"Cluster\"] = y_jerarq\n",
    "\n",
    "# Calculo la media de cada variable para cada cluster\n",
    "df.groupby([\"Cluster\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2746191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculo la desviacion de cada variable para cada cluster\n",
    "df.groupby([\"Cluster\"]).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a63f8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pop('Cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2494f8a",
   "metadata": {},
   "source": [
    "Lo anterior se podria haber resuelto con una funcion que tome como parametro el linkage pero por las limitaciones de graficacion de notebbok se decidio copiar y pegar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39de9bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": false,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": false,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
